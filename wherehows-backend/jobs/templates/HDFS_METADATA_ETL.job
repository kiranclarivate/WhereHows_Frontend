# Fetch HDFS dataset metadata

# Common ETL configs
job.class=metadata.etl.dataset.hdfs.HdfsMetadataEtl
job.cron.expr=0 0 5 ? * TUE,THU,SUN *
job.timeout=3000
#job.cmd.params=
#job.disabled=1
job.ref.id=11

# Cluster name
hdfs.cluster=your_cluster_name

# Run job on remote machine, true/false
hdfs.remote.mode=false

# Remote Hadoop gateway machine name (could be localhost)
hdfs.remote.machine=

# Private key used to log in to the remote machine
hdfs.private_key_location=

# JAR file location on the remote machine
hdfs.remote.jar=

# User login on remote machine
hdfs.remote.user=

# Metadata JSON file location on remote machine
hdfs.remote.raw_metadata=

# Sample data CSV file location on remote machine
hdfs.remote.sample=

# Place to store field metadata file
hdfs.local.field_metadata=

# Place to store metadata CSV file
hdfs.local.metadata=hdfs_metadata.csv

# Place to store metadata JSON file
hdfs.local.raw_metadata=hdfs_raw_metadata.json

# Place to store sample file
hdfs.local.sample=hdfs_sample.dat

# The whitelist of folder to collect metadata
hdfs.white_list=

# Optional. number of threads you want to scrape the HDFS
hdfs.num_of_thread=1

# Optional. The map of file path regex and dataset source. e.g. [{"/data/tracking.":"Kafka"},{"/data/retail.":"Teradata"}]
hdfs.file_path_regex_source_map=